import{j as e,l as t}from"./main-DdHQErvU.js";import{B as s}from"./layout-DvqNAMRu.js";import{E as o}from"./papers-DOhXXac1.js";import{P as n}from"./pdf-viewer-37bKVnAO.js";import"./index-Bd5IwEUQ.js";import"./index-DUESsLd7.js";import"./button-CFX2dUjs.js";const m=function(){return e.jsxs(e.Fragment,{children:[e.jsx(s,{to:t.fullPath}),e.jsx(o,{href:"https://arxiv.org/pdf/2308.03762v2"}),e.jsx("p",{children:"I understand why people anthropomorphize things like LLMs, it makes them a lot easier to talk about. But because the language being used is so non-specific, people misunderstand what these tools really are and how they work."}),e.jsx("p",{children:`I've built a few "AI" systems before, mostly computer-vision based, so I generally understand how "AI" models work.`}),e.jsx("p",{children:`LLMs, like GPT-4, are just text-based autocomplete on steroids. They're really great at predicting the next words and phrases that an actual human would use given some "context", but there's no real thinking going on under the hood.`}),e.jsxs("p",{children:["While it's tempting to point at reasoning models like"," ",e.jsx("a",{href:"https://api-docs.deepseek.com/news/news250120",children:"Deepseek's R1"}),","," ",e.jsx("b",{children:"these models are not thinking"}),", they're text-based autocomplete."]}),e.jsx("p",{children:"LLMs can still be useful even if they aren't thinking! But it's important to recognize the strengths and weaknesses of LLMs the same way we do with other technologies."}),e.jsxs("p",{children:[`Talking about LLMs as if they're "thinking" obscures their biggest weakness:`," ",e.jsx("b",{children:"LLMs provide confident answers that are often completely false based solely on statistics of most-likely occurrence of the next word or phrase."})]}),e.jsx("div",{className:"flex aspect-[800/1087] w-full justify-center md:w-[80%] lg:w-[60%]",children:e.jsx(n,{filepath:"/blog/papers/gpt4-cant-reason.pdf"})})]})};export{m as component};
