import{j as e,t as a}from"./main-DdHQErvU.js";import{S as i}from"./code-block-aPXLCB2L.js";import{B as o}from"./layout-DvqNAMRu.js";import{H as s,a as t}from"./header-D86GcD9X.js";import{S as r}from"./separator-LnusGYe2.js";import"./button-CFX2dUjs.js";const p=function(){return e.jsxs(e.Fragment,{children:[e.jsx(o,{to:a.fullPath}),e.jsx("p",{className:"mt-4",children:"During my time at ROSALIND, I managed our 4TB Postgres database for current and historical COVID-19 sequence analysis."}),e.jsx(s,{hash:"the-database",children:"The Database"}),e.jsx("p",{children:"The Postgres instance has 24 vCPUs, 128GB of RAM, and 4TB of solid-state storage. The database tables of interest include samples, sequences, and mutations:"}),e.jsx(t,{hash:"samples",children:"Samples"}),e.jsxs("ul",{children:[e.jsx("li",{children:"ID"}),e.jsxs("li",{children:["Accession number",e.jsx("ul",{children:e.jsx("li",{children:"A unique identifier (unique for the dataset provider, of which we only had GISAID)"})})]}),e.jsx("li",{children:"Collection date"}),e.jsxs("li",{children:["Collection location",e.jsx("ul",{children:e.jsx("li",{children:"Continent, country, region (states in USA), subregion (counties in USA)"})})]}),e.jsx("li",{children:"Variant/lineage"}),e.jsx("li",{children:"Basic patient info (sex, age, etc.)"})]}),e.jsx(t,{hash:"sequences",children:"Sequences"}),e.jsxs("ul",{children:[e.jsx("li",{children:"ID"}),e.jsx("li",{children:"Full DNA sequence (ATGC string)"}),e.jsxs("li",{children:[e.jsx("code",{children:"sha256"})," of the DNA sequence",e.jsx("ul",{children:e.jsxs("li",{children:["Sequences should be unique, but unique indexes are limited by page size. So we make the ",e.jsx("code",{children:"sha256"})," of the sequence unique instead"]})})]})]}),e.jsx(t,{hash:"mutations",children:"Mutations"}),e.jsxs("ul",{children:[e.jsx("li",{children:"ID"}),e.jsx("li",{children:"Start position"}),e.jsx("li",{children:"Stop position"}),e.jsxs("li",{children:["Indel",e.jsxs("ul",{children:[e.jsxs("li",{children:[e.jsx("strong",{children:"In"}),"sertion / ",e.jsx("strong",{children:"del"}),"etion"]}),e.jsx("li",{children:"The inserted/deleted/modified characters when compared to the reference sequence"})]})]})]}),e.jsx(s,{hash:"the-queries",children:"The Queries"}),e.jsx("p",{children:"The vast majority of queries were analytical queries, which looked something like this:"}),e.jsx(i,{content:`SELECT
    start,
    indel,
    count(*)
FROM
    mutations
    JOIN sequence_to_mutation ON mutations.id = sequence_to_mutation.mutation_id
    JOIN sample ON sample.sequence_id = sequence_to_mutation.sequence_id
WHERE
    sample.continent = 'North America'
    AND sample.country = 'USA'
    AND sample.state = 'California'
    AND sample.collected >= NOW() - '30 days'::interval
GROUP BY
    start,
    indel;`}),e.jsx("p",{children:"This query pulled all unique mutations for any samples from California collected within the last 30 days. This data was then used to populate the Genome Browser."}),e.jsxs("p",{children:["This query would typically return results within 5 seconds, which was acceptable for our use case. The problem was, users would regularly increase the timeframe or remove it altogether. As the number of samples matching our filters increased, so did the number of unique mutations. By August 2023, we had ~20 million samples in our database, with ~50 mutations per sample. Running this query without ",e.jsx("em",{children:"any"})," ","filters took >2 minutes to fully complete."]}),e.jsx("p",{children:"We had a few solutions in mind:"}),e.jsxs("ol",{children:[e.jsx("li",{children:"Cache the results once per day. Because we only ingested new data once a day, we could rebuild the cache after that day's data ingestion. This worked great for existing queries, but didn't solve the problem for new queries"}),e.jsxs("li",{children:["Denormalize data. We could store the mutations on the samples table in an"," ",e.jsx("code",{children:"ARRAY"})," column, saving us the ",e.jsx("code",{children:"JOIN"}),"s between tables"]}),e.jsx("li",{children:"Use a different database"})]}),e.jsx("p",{children:"We explored all three options. Option 1 was trivial to implement, so we did that first. Option 2 would require a LOT of extra work to denormalize data for different queries, so we pushed that out. That left us with Option 3."}),e.jsx(s,{hash:"testing-different-databases",children:"Testing Different Databases"}),e.jsx("p",{children:"We had a few different databases we wanted to try:"}),e.jsxs("ol",{children:[e.jsxs("li",{children:[e.jsx("a",{href:"https://duckdb.org/",children:"DuckDB"}),", a columnar based database that's conceptually similar to SQLite"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://cloud.google.com/products/alloydb",children:"AlloyDB"}),", a Postgres-compatible database from GCP that's built with analytical queries in mind"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://cloud.google.com/bigquery",children:"BigQuery"}),", a GCP service used for terabyte/petabyte scale data analytics"]})]}),e.jsx("p",{children:"Unfortunately, the exact tests and results have been lost to time. Maybe one day I'll replicate this testing, but for now, here's an overview."}),e.jsx(t,{hash:"duckdb",children:"DuckDB"}),e.jsxs("p",{children:["DuckDB was very easy to get started with. We first extracted all of the relevant data into a handful of CSV files. We then transformed those CSV files into"," ",e.jsx("a",{href:"https://www.databricks.com/glossary/what-is-parquet",children:"Parquet files"}),". Parquet files are similar to CSVs, but encode data in a column-based format instead of a row-based format. We then installed the"," ",e.jsx("a",{href:"https://duckdb.org/docs/api/python/overview.html",children:"Python client"})," and tested a handful of queries."]}),e.jsx("p",{children:"We saw great results with DuckDB. We were pleasantly pleased with how fast the analytical queries were. The downside was that managing a DuckDB service looked moderately challenging. We'd need to export data to Parquet files, make those files accessible to servers running DuckDB, and put an API in front of our DuckDB system. Because of that, we decided to step away from self-hosted systems and look only at managed systems."}),e.jsx(t,{hash:"alloydb",children:"AlloyDB"}),e.jsxs("p",{children:["AlloyDB was a bit more challenging to get set up. GCP's database migration tool didn't work with AlloyDB at the time, so we used the"," ",e.jsx("a",{href:"https://www.postgresql.org/docs/current/postgres-fdw.html",children:"Foreign Data Wrapper (FDW) extension"})," ","to copy data over from our main Postgres database. Once data was in AlloyDB, we"," ",e.jsx("a",{href:"https://cloud.google.com/alloydb/docs/columnar-engine/manage-content-manually",children:"manually configured the columnar engine"})," ","for our tables of interest."]}),e.jsx("p",{children:"While the results were promising, we decided to stop testing with Alloy DB when we realized it wasn't yet FedRAMP compliant. ROSALIND was a FedRAMP Moderate compliant platform, so we could only use GCP products that were FedRAMP Moderate or High. As of December 2024, Alloy DB is under review for FedRAMP High."}),e.jsx(t,{hash:"bigquery",children:"BigQuery"}),e.jsxs("p",{children:["BigQuery was also challenging to get set up. GCP's Datastream product does allow you to"," ",e.jsx("a",{href:"https://cloud.google.com/datastream/docs/destination-bigquery",children:"sync data from Cloud SQL Postgres to BigQuery"}),", but we ran into one major issue:"]}),e.jsx("ul",{children:e.jsxs("li",{children:["Datastream didn't allow backfilling tables that had more than 100 million rows",e.jsxs("ul",{children:[e.jsxs("li",{children:["Since"," ",e.jsx("a",{href:"https://cloud.google.com/datastream/docs/release-notes#May_11_2023",children:"May 11, 2023"}),","," ",e.jsx("a",{href:"https://cloud.google.com/datastream/docs/release-notes#May_11_2023",children:"this limitation has been lifted"})]}),e.jsx("li",{children:"Because of this limitation, we had to manually import data into BigQuery, which was extremely fiddly and error-prone"})]})]})}),e.jsxs("p",{children:["Once data was imported into BigQuery, we could either use"," ",e.jsx("a",{href:"https://github.com/googleapis/python-bigquery-sqlalchemy",children:"a SQLAlchemy plugin"})," ","or raw queries to query the database. We started with raw queries, as we already had support for that in our backend. IIRC, nearly every query to BigQuery completed in ~10 seconds or less. BigQuery also cached results automatically, and invalidated the cache daily."]}),e.jsx("p",{children:"While this was great for performance, we saw that BigQuery was scanning ~15GB of data for every query. With some basic math, we found that every unique query was costing us ~$0.09375 (about 9 cents). That's totally fine for testing, but not something we were okay with for production traffic."}),e.jsx("p",{children:"On average, we saw a few hundred queries for the Genome Browser every day. Assuming we get 200 unique Genome Browser queries a day, that equates to ~$19 a day. Not too bad! But we were also looking to add more queries into this system, which would've driven up the cost by an order of magnitude. With a total estimated cost of over $200 a day, we just weren't comfortable paying for yet another database."}),e.jsx(r,{}),e.jsx(s,{hash:"end-result",children:"End Result"}),e.jsx("p",{children:"In the end, we decided to denormalize data and fit the denormalized tables to our access patterns. I hated this solution then and I still hate this solution today, but it did solve the problems we were having with query times. Keeping those denormalized tables up-to-date was challenging and took a lot of engineering effort. Now that AlloyDB is (in the process of becoming) FedRAMP compliant, that seems like the obvious choice to redo this project."})]})};export{p as component};
